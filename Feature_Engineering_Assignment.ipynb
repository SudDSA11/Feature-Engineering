{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a parameter?"
      ],
      "metadata": {
        "id": "3EbaJ-xPMuGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "A parameter is an internal variable that a model learns from the training data to make predictions.\n",
        "Example: In linear regression, the slope (weights) and intercept (bias) are parameters.\n",
        "'''"
      ],
      "metadata": {
        "id": "J952kGBAMxMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is correlation?\n",
        "   What does negative correlation mean?"
      ],
      "metadata": {
        "id": "kSeoW6KNNAA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Correlation is a statistical measure that describes the strength and direction of the relationship between two variables.\n",
        "It is represented by the correlation coefficient (r), which ranges from -1 to +1.\n",
        "A negative correlation means that when one variable increases, the other tends to decrease.\n",
        "The correlation coefficient (r) lies between -1 and 0.\n",
        "'''"
      ],
      "metadata": {
        "id": "oJqibhgVNE0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Define Machine Learning. What are the main components in Machine Learning?"
      ],
      "metadata": {
        "id": "GqZXlWxOgbzk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFmgrXCbgQu7"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Machine Learning is defined as Learning of patterns from the data and replicate the saame in  future\n",
        "The main components of machine learnings are:\n",
        "1. Data\n",
        "2. Features\n",
        "3. Model\n",
        "4. Training\n",
        "5. Training\n",
        "6. Evaluation\n",
        "7. Finally Prediction of the Target variable\n",
        "Machine Learning = Data + Features + Model + Training + Evaluation → Predictions\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. How does loss value help in determining whether the model is good or not?\n"
      ],
      "metadata": {
        "id": "Q7lnnAlhNYbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "In machine learning, loss is a number that measures how far the model’s predictions are from the actual values.\n",
        "How Loss Value Helps Judge a Model:-\n",
        "1.Training Progress\n",
        "  During training, if the loss keeps decreasing, it means the model is learning well.\n",
        "  If the loss stops decreasing or goes up, the model may be overfitting or underfitting.\n",
        "2.Comparing Models\n",
        "  You can train different models (e.g., Linear Regression vs Random Forest) and compare their losses.\n",
        "  The one with the lower validation/test loss is usually better.\n",
        "3.Generalization Check\n",
        "'''"
      ],
      "metadata": {
        "id": "VZvOWfpbNhFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.What are continuous and categorical variables?"
      ],
      "metadata": {
        "id": "HYXcwMVXOO7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Continuous Variables\n",
        "These are numeric variables that can take any value within a range.\n",
        "They are measurable (not countable).\n",
        "Examples:-\n",
        "Height (170.5 cm, 171.2 cm, …)\n",
        "\n",
        "Categorical Variables\n",
        "These represent categories or groups, not continuous numbers.\n",
        "They are qualitative rather than quantitative.\n",
        "Two types:\n",
        "Nominal → No natural order between categories.\n",
        "Example: Colors (Red, Blue, Green), Gender (Male, Female).\n",
        "Ordinal → Categories with a natural order/ranking\n",
        "Example: Education level (High School < Bachelor < Master < PhD), Ratings (Poor < Average < Good)."
      ],
      "metadata": {
        "id": "CZYmGKWKOX61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. How do we handle categorical variables in Machine Learning? What are the common techniques?\n"
      ],
      "metadata": {
        "id": "TFt5SHJhjeJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Handling categorical variables is one of the most important steps in ML preprocessing, since most algorithms expect numeric inputs.\n",
        "1. Label Encoding\n",
        "  Assigns each category a unique integer.\n",
        "  Works well for tree-based models (Decision Trees, Random Forest, XGBoost).\n",
        "  Not good for linear models → it introduces a false ordinal relationship.\n",
        "2. One-Hot Encoding\n",
        "   Creates new binary columns (dummy variables) for each category.\n",
        "   Best for linear models, logistic regression, neural networks.\n",
        "   Can cause high dimensionality if categories are many.\n",
        "3. Ordinal Encoding\n",
        "   Encodes categories based on order (if it exists).\n",
        "   Works when the variable has natural ranking.\n",
        "   Wrong to use if no true order exists.\n",
        "4. Target (Mean) Encoding\n",
        "   Replace category with the mean of target variable.\n",
        "   Useful for high-cardinality features (hundreds of categories).\n",
        "   Risk of data leakage → use with cross-validation\n",
        "'''"
      ],
      "metadata": {
        "id": "h1boquwRjnHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What do you mean by training and testing a dataset?"
      ],
      "metadata": {
        "id": "uAGu7miWmq_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "The full data set is divide on training and test dataset given a particualr proprtion to each as we can use full dataset solely for model training , the model needs\n",
        "to get some unseen data to test the model.\n",
        "The training dataset is bigger subset of the data set on which the model is trained and validated.\n",
        "The test dataset is smaller subset of the data onn which the model is tested to find its accuracy\n",
        "'''"
      ],
      "metadata": {
        "id": "d2P7QZNRms_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is sklearn.preprocessing?"
      ],
      "metadata": {
        "id": "ls6DRuDwn7Fi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Sklearn.preprocessing is module in the library scikit learn library that provides tools for preparing and transforming raw data into a\n",
        "suitable format for machine learning models.\n",
        "'''\n"
      ],
      "metadata": {
        "id": "cmmNgY5Tn-_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is a Test set?"
      ],
      "metadata": {
        "id": "M4WisPHqoXEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "A test set is a portion of your dataset that is set aside and not used during training.\n",
        "It is used only after the model is trained to evaluate how well the model performs on unseen data.\n",
        "Purpose of a Test Set is :-\n",
        "To check if the model can generalize (perform well on new data, not just memorize training data).\n",
        "To estimate the model’s real-world performance.\n",
        "To detect problems like overfitting (when the model memorizes training data but fails on unseen data).\n",
        "'''"
      ],
      "metadata": {
        "id": "TxyNHjJ-ofiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.How do we split data for model fitting (training and testing) in Python?\n",
        "   How do you approach a Machine Learning problem?"
      ],
      "metadata": {
        "id": "gtit-jWMPF-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "We split data into Training set (to train the model) and Testing set (to evaluate performance on unseen data).\n",
        "Sometimes we also use a Validation set (for hyperparameter tuning).\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Suppose X = features, y = target\n",
        "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=1)\n",
        "print(X_train.shape, X_test.shape)\n",
        "\n",
        "Approcahing the Machine Learning requies several steps which are as follows:-\n",
        "1. Understanding the problem\n",
        "2. Data Ingestion\n",
        "3. Exploratory Data Analysis\n",
        "4. Preparation of Data : Data cleaning, Feature engineering etc\n",
        "5. Split the data into train-test split\n",
        "6. Scaling(optional)\n",
        "7. Model training\n",
        "8. Model evaluation\n",
        "'''"
      ],
      "metadata": {
        "id": "REdsPAh_PItX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Why do we have to perform EDA before fitting a model to the data?"
      ],
      "metadata": {
        "id": "DY1PuCdzQ6bi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "EDA is the first deep dive into the data — it helps us understand the dataset, detect problems, and prepare it for modeling.\n",
        "Reasons for Doing EDA:-\n",
        "1.Understand Data Structure\n",
        "2.Detect Missing Values\n",
        "3.Spot Outliers\n",
        "4.Check Data Distribution\n",
        "5.Understand Relationships Between Features\n",
        "6.Feature Engineering Ideas\n",
        "'''"
      ],
      "metadata": {
        "id": "u1PfIdLpQ8AS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.What is correlation?"
      ],
      "metadata": {
        "id": "ZaRUJb3VRjTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Correlation is a statistical measure that describes the strength and direction of the relationship between two variables.\n",
        "It is represented by the correlation coefficient (r), which ranges from -1 to +1.\n",
        "'''"
      ],
      "metadata": {
        "id": "9b5Q4k7gRnf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.What does negative correlation mean?"
      ],
      "metadata": {
        "id": "5seJBTFKRtF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "A negative correlation means that when one variable increases, the other tends to decrease.\n",
        "The correlation coefficient (r) lies between -1 and 0.\n",
        "'''"
      ],
      "metadata": {
        "id": "Ok1vje6ZRypk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "14.How can you find correlation between variables in Python?"
      ],
      "metadata": {
        "id": "48xUMJutR3vM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "There are several ways to find the correaltion in python:-\n",
        "1. corr() function\n",
        "2.visualization the heatmap with seaborn\n",
        "'''"
      ],
      "metadata": {
        "id": "Q6RrzdRBSQyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. What is causation? Explain difference between correlation and causation with an example.\n"
      ],
      "metadata": {
        "id": "pJ0_Y4jUS1K2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Causation\n",
        "Causation means that one variable directly affects another.\n",
        "If X causes Y → a change in X will produce a change in Y.\n",
        "example:- if the number of hours studied is more then marks scored in exams is more(causation)\n",
        "\n",
        "Correlation\n",
        "Correlation means two variables are related/move together, but it doesn’t prove that one causes the other.\n",
        "example:- Taller people wear bigger shoes (correlation)\n",
        "'''"
      ],
      "metadata": {
        "id": "ewdm9jKbS7Yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "16.What is an Optimizer? What are different types of optimizers? Explain each with an example."
      ],
      "metadata": {
        "id": "nOpJXHjtTpGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "An optimizer is an algorithm used in training ML/DL models to update the model’s parameters (weights & biases) in order to minimize the loss function.\n",
        "It decides how fast and in what direction the model should move in the parameter space to reach the best solution.\n",
        "Types of Optimizers:-\n",
        "1.Gradient Descent (GD)\n",
        "  example:-Linear Regression training.\n",
        "2.Stochastic Gradient Descent (SGD)\n",
        "  example:-raining a Neural Network with SGD.\n",
        "3.Mini-Batch Gradient Descent\n",
        "  it is a  compromise between GD and SGD.\n",
        "4.Momentum\n",
        "  Example: Like pushing a ball downhill — it gains speed (momentum).\n",
        "'''"
      ],
      "metadata": {
        "id": "V82892XnTq4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.What is sklearn.linear_model ?"
      ],
      "metadata": {
        "id": "Ja_5zr1fUca_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "sklearn.linear_model is a module in scikit-learn (sklearn) that provides implementations of linear models for both regression and classification problems.\n",
        "These models assume a linear relationship between input features (X) and output (y).\n",
        "'''"
      ],
      "metadata": {
        "id": "qnq8r5R7Ue3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "18.What does model.fit() do? What arguments must be given?"
      ],
      "metadata": {
        "id": "VvTEjKijUoY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "The .fit() method is used to train a model in scikit-learn (and other ML libraries).\n",
        "It takes in the training data (features + target) and learns the best parameters (weights, intercept, etc.) to minimize the loss function.\n",
        "Argumnets given are X_train, y_train\n",
        "'''"
      ],
      "metadata": {
        "id": "Zcz6AI2CUwg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "19.What does model.predict() do? What arguments must be given?\n"
      ],
      "metadata": {
        "id": "PJNRk7_PUt6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "After you train a model with .fit(), the .predict() method is used to make predictions on new (unseen) data.\n",
        "It applies the learned parameters (weights, bias, coefficients, etc.) to the input features and outputs the predicted values.\n",
        ".predict() = use what was learned to make predictions\n",
        "Arguments given is X_test\n",
        "and it return y_pred.\n",
        "'''"
      ],
      "metadata": {
        "id": "zjtVmmhEVPsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "20 What are continuous and categorical variables?"
      ],
      "metadata": {
        "id": "1uhg0NKJVqFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Continuous variables\n",
        "Numeric values that can take any value within a range (including decimals).\n",
        "Measurable quantities.\n",
        "Examples: height (170.5 cm), weight (65.3 kg), temperature (23.1 °C), salary.\n",
        "ML use: often used in regression; usually scaled (StandardScaler / MinMaxScaler) before modelling.\n",
        "Visualizations: histogram, boxplot, density plot.\n",
        "\n",
        "Categorical variables\n",
        "Discrete labels or groups (not inherently numeric).\n",
        "Two subtypes:\n",
        "Nominal — no natural order (e.g., color: red/blue/green; country).\n",
        "Ordinal — with order (e.g., education level: High School < Bachelor < Master < PhD; rating: low/medium/high).\n",
        "ML use: must be encoded to numbers (one-hot, label/ordinal, target/frequency encoding) before most models.\n",
        "Visualizations: bar chart, countplot.\n",
        "'''"
      ],
      "metadata": {
        "id": "U1u5pQTvVsIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What is feature scaling? How does it help in Machine Learning?"
      ],
      "metadata": {
        "id": "UUiKBNDpWK8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Feature scaling is the process of bringing all features (columns) in a dataset to a similar scale (range or distribution).\n",
        "Many datasets have features with very different units:\n",
        "Example:\n",
        "Age → 18–70\n",
        "Salary → 20,000 – 1,00,000\n",
        "Height → 150 – 200 cm\n",
        "If we don’t scale, models that rely on distance or gradient descent may give more importance to larger-scale features.\n",
        "\n",
        "Feature Scaling helps :-\n",
        "1.Prevents domination by large-scale features\n",
        "2.Improves convergence in gradient-based algorithms\n",
        "3.Essential for distance-based algorithms\n",
        "4.Makes coefficients more interpretable\n",
        "'''"
      ],
      "metadata": {
        "id": "EFCxG0O9WQrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "22.How do we perform scaling in Python?"
      ],
      "metadata": {
        "id": "4iuOIqSQWyDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Scaling in Python is usually done with scikit-learn’s preprocessing module.\n",
        "1.Standardization (Z-score Scaling)\n",
        " from sklearn.preprocessing import StandardScaler\n",
        " scaler = StandardScaler()\n",
        " scaled_data = scaler.fit_transform()\n",
        "2.Min-Max Scaling (Normalization)\n",
        " from sklearn.preprocessing import MinMaxScaler\n",
        " scaler = MinMaxScaler()\n",
        " scaled_data = scaler.fit_transform()\n",
        "3.Robust Scaling (Handles Outliers)\n",
        " from sklearn.preprocessing import RobustScaler\n",
        " scaler = RobustScaler()\n",
        " scaled_data = scaler.fit_transform()\n",
        "'''\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UsQj1t-4W2_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "23.What is sklearn.preprocessing?"
      ],
      "metadata": {
        "id": "C-7vMWKOXkDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Sklearn.preprocessing is module in the library scikit learn library that provides tools for preparing and transforming raw data into a\n",
        "suitable format for machine learning models.\n",
        "'''\n"
      ],
      "metadata": {
        "id": "43T6EYZkXluT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. How do we split data for model fitting (training and testing) in Python?\n"
      ],
      "metadata": {
        "id": "1om_9-FGXslL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Example dataset\n",
        "data = pd.DataFrame({\n",
        "    'Age': [20, 25, 30, 35, 40, 45, 50],\n",
        "    'Salary': [20000, 30000, 40000, 50000, 60000, 70000, 80000],\n",
        "    'Purchased': [0, 0, 0, 1, 1, 1, 1]  # Target variable\n",
        "})\n",
        "\n",
        "X = data[['Age', 'Salary']]   # Features\n",
        "y = data['Purchased']         # Target\n",
        " Split data (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split( X, y,test_size=0.2,random_state=1)\n",
        "\n",
        "print(\"Train Features:\\n\", X_train)\n",
        "print(\"Test Features:\\n\", X_test)\n",
        "print(\"Train Target:\\n\", y_train)\n",
        "print(\"Test Target:\\n\", y_test)\n",
        "'''"
      ],
      "metadata": {
        "id": "745g67I5XxfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "25.Explain data encoding?"
      ],
      "metadata": {
        "id": "ByTx9mKBYp7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Data encoding is the process of converting these categorical values into numeric representations that ML models can understand.\n",
        "Types of Data Encoding:-\n",
        "1. Label Encoding\n",
        "  Converts categories into integer values.\n",
        "2.One-Hot Encoding\n",
        "  Creates binary columns for each category.\n",
        "3.Ordinal Encoding\n",
        "  Assigns numbers according to order (if present).\n",
        "4.Target / Mean Encoding (for advanced ML)\n",
        "  Replaces categories with the mean of the target variable for that category.\n",
        "'''"
      ],
      "metadata": {
        "id": "9GuCQrbEYt0r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}